---
title: "NAM: Normalization-based Attention Module"
collection: publications
permalink: /publication/2021-yichao-nam
excerpt: 'we propose a novel normalization-based attention module (NAM), which suppresses less salient weights. It applies a weight sparsity penalty to the attention modules, thus, making them more computational efficient while retaining similar performance.'
date: 2021
venue: 'ImageNet PPF @ NeurIPS 2021'
paperurl: 'https://arxiv.org/abs/2111.12419'
citation: 'Liu, Y., Shao, Z., Teng, Y., Hoffmann N. (2021). NAM: Normalization-based Attention Module. ImageNet PPF @ NeurIPS 2021.'
---

We propose a novel normalization-based attention module (NAM), which suppresses less salient weights. It applies a weight sparsity penalty to the attention modules, thus, making them more computational efficient while retaining similar performance.

[Download paper here](https://arxiv.org/abs/2111.12419)

Recommended citation: Liu, Y., Shao, Z., Teng, Y., Hoffmann N. (2021). NAM: Normalization-based Attention Module. ImageNet PPF @ NeurIPS 2021.
